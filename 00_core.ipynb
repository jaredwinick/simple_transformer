{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp simple_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple_transformer\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# https://github.com/huggingface/transformers/blob/master/transformers/modeling_tf_utils.py\n",
    "def shape_list(x):\n",
    "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(x)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def maskTF(matrices, mask_value=0.0, mask_diagonal=True):\n",
    "  b, h, w = matrices.shape\n",
    "  assert h == w\n",
    "\n",
    "  mask = (1 - tf.linalg.band_part(tf.ones((h, h)), -1, 0)) * tf.float32.min\n",
    "  masked = matrices - mask\n",
    "  \n",
    "  return masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SelfAttentionTF(tf.keras.Model):\n",
    "  def __init__(self, emb, heads, mask=False):\n",
    "    super(SelfAttentionTF, self).__init__()\n",
    "\n",
    "    self.emb = emb\n",
    "    self.heads = heads\n",
    "    self.mask = mask\n",
    "\n",
    "    self.tokeys = tf.keras.layers.Dense(emb * heads, input_shape=(emb,), use_bias=False)\n",
    "    self.toqueries = tf.keras.layers.Dense(emb * heads, input_shape=(emb,), use_bias=False)\n",
    "    self.tovalues = tf.keras.layers.Dense(emb * heads, input_shape=(emb,), use_bias=False)\n",
    "    self.unifyheads = tf.keras.layers.Dense(emb, input_shape=(emb * heads,))\n",
    "\n",
    "  def call(self, x):\n",
    "    b, t, e = shape_list(x)\n",
    "    h = self.heads\n",
    "    assert e == self.emb, f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'\n",
    "\n",
    "    keys = tf.reshape(self.tokeys(x), [b, t, h, e])\n",
    "    queries = tf.reshape(self.toqueries(x), [b, t, h, e])\n",
    "    values = tf.reshape(self.tovalues(x), [b, t, h, e])\n",
    "    # compute scaled dot-product self-attention\n",
    "\n",
    "    # - fold heads into the batch dimension\n",
    "    keys = tf.reshape( tf.transpose(keys, perm=[0, 2, 1, 3]), [b * h, t, e] )\n",
    "    queries = tf.reshape( tf.transpose(queries, perm=[0, 2, 1, 3]), [b * h, t, e] )\n",
    "    values = tf.reshape( tf.transpose(values, perm=[0, 2, 1, 3]), [b * h, t, e] )\n",
    "\n",
    "    queries = queries / (e ** (1/4))\n",
    "    keys    = keys / (e ** (1/4))\n",
    "    # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
    "    #   This should be more memory efficient\n",
    "\n",
    "    # - get dot product of queries and keys, and scale\n",
    "    dot = tf.matmul(queries, tf.transpose(keys, perm=[0, 2, 1]))\n",
    "    #assert 1 == 1\n",
    "    #assert shape_list(dot) == [b * h, t, t]\n",
    "\n",
    "    if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal\n",
    "      dot = maskTF(dot, mask_value=float('-inf'), mask_diagonal=False)\n",
    "\n",
    "    dot = tf.nn.softmax(dot, axis=2)\n",
    "    # - dot now has row-wise self-attention probabilities\n",
    "\n",
    "    # apply the self attention to the values\n",
    "    out = tf.reshape(tf.matmul(dot, values), [b, h, t, e])\n",
    "\n",
    "    # swap h, t back, unify heads\n",
    "    out = tf.reshape(tf.transpose(out, perm=[0, 2, 1, 3]), [b, t, h * e])\n",
    "\n",
    "    return self.unifyheads(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerBlockTF(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, emb, heads, mask, seq_length, ff_hidden_mult=4, dropout=0.0):\n",
    "    super(TransformerBlockTF, self).__init__()\n",
    "\n",
    "    self.attention = SelfAttentionTF(emb, heads=heads, mask=mask)\n",
    "    self.mask = mask\n",
    "\n",
    "    self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "    self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    self.ff = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(ff_hidden_mult * emb, input_shape=(emb,), activation='relu'),\n",
    "      tf.keras.layers.Dense(emb)\n",
    "    ])\n",
    "\n",
    "    self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "  def call(self, x):\n",
    "    attended = self.attention(x)\n",
    "    x = self.norm1(attended + x)\n",
    "    x = self.do(x)\n",
    "    fedforward = self.ff(x)\n",
    "    x = self.norm2(fedforward + x)\n",
    "    x = self.do(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"dense_70_input:0\", shape=(None, 3), dtype=float32) for input (None, 3), but it was re-called on a Tensor with incompatible shape (2, 2, 3).\n"
     ]
    }
   ],
   "source": [
    "model = TransformerBlockTF(emb=3, heads=2, mask=False, seq_length=2)\n",
    "xb = tf.constant([[[.5, .4, .1], [.3, .4, .1]], [[.2, .3, .3], [.9, .2, .54]]])\n",
    "out = model(xb)\n",
    "b, t, e = out.shape\n",
    "assert b == 2\n",
    "assert t == 2\n",
    "assert e == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CTransformerTF(tf.keras.Model):\n",
    "    def __init__(self, emb, heads, depth, seq_length, num_tokens, num_classes, max_pool=True, dropout=0.0):\n",
    "        super(CTransformerTF, self).__init__()\n",
    "        \n",
    "        self.num_tokens, self.max_pool = num_tokens, max_pool\n",
    "        self.token_embedding = tf.keras.layers.Embedding(num_tokens, emb, input_length=seq_length)\n",
    "        self.pos_embedding = tf.keras.layers.Embedding(seq_length, emb, input_length=seq_length)\n",
    "        \n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(TransformerBlockTF(emb=emb, heads=heads, seq_length=seq_length, mask=False, dropout=dropout))\n",
    "        \n",
    "        self.tblocks = tf.keras.Sequential(tblocks)\n",
    "        self.toprobs = tf.keras.layers.Dense(num_classes, input_shape=(emb,))\n",
    "        #self.do = \n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A batch by sequence length integer tensor of token indices.\n",
    "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
    "        \"\"\"\n",
    "        tokens = self.token_embedding(x)\n",
    "        b, t, e = shape_list(tokens)\n",
    "        \n",
    "        positions = tf.tile(self.pos_embedding(tf.range(t))[None, :, :], [b, 1, 1])\n",
    "        x = tokens + positions\n",
    "        #dropout\n",
    "        \n",
    "        x = self.tblocks(x)\n",
    "        #TODO max pooling vs. ave pooling\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "        x = self.toprobs(x)\n",
    "        return tf.nn.softmax(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"dense_102_input:0\", shape=(None, 3), dtype=float32) for input (None, 3), but it was re-called on a Tensor with incompatible shape (3, 3, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"dense_108_input:0\", shape=(None, 3), dtype=float32) for input (None, 3), but it was re-called on a Tensor with incompatible shape (3, 3, 3).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[0.5087288 , 0.49127123],\n",
       "       [0.49395216, 0.50604784],\n",
       "       [0.4897512 , 0.5102488 ]], dtype=float32)>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CTransformerTF(emb=3, heads=2, depth=2, seq_length=3, num_tokens=5, num_classes=2)\n",
    "xb = tf.constant([[0, 4, 1], [1, 3, 2], [2, 3, 3]])\n",
    "out = model(xb)\n",
    "batch, classes = shape_list(out)\n",
    "assert batch == 3\n",
    "assert classes == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
